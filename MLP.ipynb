{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9bVgvMKGbRtW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701121426464,"user_tz":-240,"elapsed":1310,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}},"outputId":"31bf81a4-7d51-4996-c4a6-bc8217deb2f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-11-27 21:43:45--  https://raw.githubusercontent.com/Amrtamer711/Shakespeare-Transformer/main/shakespeare_more.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5617411 (5.4M) [text/plain]\n","Saving to: ‘shakespeare_more.txt’\n","\n","shakespeare_more.tx 100%[===================>]   5.36M  --.-KB/s    in 0.1s    \n","\n","2023-11-27 21:43:45 (54.6 MB/s) - ‘shakespeare_more.txt’ saved [5617411/5617411]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/Amrtamer711/Shakespeare-Transformer/main/shakespeare_more.txt"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F # loading libraries"],"metadata":{"id":"M2QCn5LZM8W0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive') # mounting drive where files are stored"],"metadata":{"id":"W8hQnrWGcgiE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701121452924,"user_tz":-240,"elapsed":21082,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}},"outputId":"1924043d-2412-4a98-dd92-df588708c249"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["with open(r'shakespeare_more.txt', 'r', encoding='utf-8') as file:\n","    text = file.read() # reading the text from the dataset\n","unique_chars = sorted(list(set(text))) # creating a list of all the unique possible characters, the vocabulary\n","vocab_size = len(unique_chars) # checking the length of the vocabulary"],"metadata":{"id":"HstnTCnZbU3L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["itos = {i:s for i, s in enumerate(unique_chars)} # creating a dictionary that converts integer index to corresponding token string character\n","stoi = {s:i for i, s in enumerate(unique_chars)} # creating a dictionary that converts token string character to corresponding integer index\n","encode = lambda x: [stoi[char] for char in x] # creating a lambda function that takes some charcaters/tokens and converts them into their corresponding integer indices\n","decode = lambda x: ''.join([itos[index] for index in x]) # creating a lambda function that takes some integer indices and converts them into their corresponding charcaters/tokens\n","data = torch.tensor(encode(text), dtype=torch.long) # converts the whole text into their corresponding token integer indices\n","n1 = int(len(data) * 0.8)\n","n2 = int(len(data) * 0.9)\n","data_train = data[:n1]\n","data_val = data[n1:n2]\n","data_test = data[n2:] # splitting the data set into a (train, val, test) split of (80, 10, 10)"],"metadata":{"id":"SCaP3CjLbVGf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","batch_size = 96\n","context_size = 256\n","vector_length = 490\n","shapes = [context_size*vector_length] + [5000, 5000, 5000] + [vocab_size]\n","num_layers = 3\n","dropout = 0.2\n","iterations = 30000\n","eval_interval = 500\n","eval_batch = 200\n","lr = 3e-4 # hyper-parameters for MLP model"],"metadata":{"id":"eiK78sG6bc2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.is_available() # checking that GPU is usable"],"metadata":{"id":"4Ywe_QCaLd_u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","    def __init__(self, fan_in, fan_out):\n","        super().__init__()\n","        self.fwd = nn.Sequential(nn.Linear(fan_in, fan_out), nn.BatchNorm1d(fan_out), nn.ReLU(), nn.Dropout(dropout)) # creating linear layer architecture of each individual layer\n","    def forward(self, x):\n","        self.out = self.fwd(x) # applying input through linear layer\n","        return self.out\n","\n","class MLP(nn.Module):\n","    def __init__(self, shapes):\n","        super().__init__()\n","        self.char_embedding = nn.Embedding(vocab_size, vector_length) # creating character embedding look up table\n","        self.pos_embedding = nn.Embedding(context_size, vector_length) # creating positional embedding look up table\n","        self.layers = []\n","        for i in range(len(shapes)-2): # iterating through shapes of each layer of MLP\n","            self.layers.append(FeedForward(shapes[i], shapes[i+1])) # adding layer to model architecture\n","        self.layers.append(nn.Linear(shapes[-2], shapes[-1])) # adding final layer (will not include batch norm, ReLU and dropout)\n","        self.fwd = nn.Sequential(*self.layers)\n","    def forward(self, x, targets=None):\n","        B, T = x.shape # getting tensor shape input\n","        char_token = self.char_embedding(x) # getting character embedding vectors of input text\n","        pos_token = self.pos_embedding(torch.arange(T, device=device)) # getting positional embedding vectors of input text\n","        token = char_token + pos_token # getting final input by adding charcater and positional token embedding\n","        input = token.view(B, -1) # stretching the second and third layer\n","        logits = self.fwd(input) # applying input through MLP\n","        if targets == None: # checking if in inference or training\n","            loss = None # if inference, no loss is needed\n","        else: # if training...\n","            loss = F.cross_entropy(logits, targets) # applying cross-entropy loss function\n","        return logits, loss\n","    def generate(self, idx, max_length): # function to generate text after training\n","        for _ in range(max_length): # ensuring output is of requested length\n","            idx_block = idx[:, -context_size:] # take last context window length of tokens from complete context\n","            logits, loss = self(idx_block) # applying context window as input into MLP\n","            probs = F.softmax(logits, dim=-1) # apply softmax on logits to create probability distirbution\n","            char = torch.multinomial(probs, num_samples=1) # use weighted sampling to extract next token\n","            idx = torch.cat((idx, char), dim=1) # concatenate output character/token to previous text\n","        return idx"],"metadata":{"id":"uNS-S0EFFUZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad()\n","def estimate_loss(): # function to estimate train and val loss while training\n","    out = {}\n","    model.eval() # setting model in evaluation mode\n","    for i in ['train', 'val']: # running over train and val datasets\n","        losses = torch.zeros(eval_batch) # creating lossses tensor of set evaluation batch length hyper-parameter\n","        for j in range(eval_batch): # running over losses tensor\n","            X_batch, Y_batch = get_batch(i) # get batch of inputs and expected outputs\n","            logits, loss = model(X_batch, Y_batch) # run inputs and expected outputs through model to get loss\n","            losses[j] = loss.item() # add value to losses tensor\n","        out[i] = losses.mean() # get mean of losses tensor and save it\n","    model.train() # put model back in train mode\n","    return out\n","\n","@torch.no_grad()\n","def get_batch(mode): # function to collect batch of input/output pairs for training\n","    data = data_train if mode == 'train' else data_val # retrieve appropriate dataset\n","    batch = torch.randint(len(data) - context_size, (batch_size,)) # getting batches of random integers that will be used as dataset token integer indices of the starting token in the context window\n","    X_batch = torch.stack([data[i:i+context_size] for i in batch]) # stacking batches of context token indices to be used as input\n","    Y_batch = torch.stack([data[i+context_size] for i in batch]) # stacking batches of next character token index as output\n","    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device) # moving batch to GPU\n","    return X_batch, Y_batch\n","\n","@torch.no_grad()\n","def save_params(model, optimizer, scheduler): # function to save parameters, optimizer and scheduler states\n","  torch.save(model.state_dict(), r'/content/drive/MyDrive/ML_project/params.pt') # to save parameters\n","  torch.save(optimizer.state_dict(), r'/content/drive/MyDrive/ML_project/optimizer.pt') # to save optimizer state\n","  torch.save(scheduler.state_dict(), r'/content/drive/MyDrive/ML_project/scheduler.pt') # to save scheduler states\n","\n","@torch.no_grad()\n","def test_model(model, data, batch_size): # function to test model on test set loss and accuracy\n","    cost = []\n","    accuracy = []\n","    for i in range(0, len(data) - context_size - batch_size, batch_size): # running model through whole test dataset\n","        X_batch = torch.stack([data[j:j+context_size] for j in range(i, i + batch_size)]) # getting test input batch\n","        Y_batch = torch.stack([data[j+context_size] for j in range(i, i + batch_size)]) # getting test expected output batch\n","        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device) # moving batches to GPU\n","        logits, loss = model(X_batch, Y_batch) # running model through input/output batch\n","        cost.append(round(loss.item(), 4)) # get loss for batch and store it\n","        probs = F.softmax(logits, dim=-1) # apply softmax to get probability distirbution of next character token\n","        char = torch.multinomial(probs, num_samples=1).view(-1) # preform weighted sampling\n","        accuracy.append((len(char[char == Y_batch]) / 300) * 100) # check how common are expected and actual output equal\n","    test_cost = sum(cost) / len(cost) # calculate average test loss\n","    test_accuracy = sum(accuracy) / len(accuracy) # calculate average test accuracy\n","    return test_cost, test_accuracy"],"metadata":{"id":"cM1j0rQOFXc8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = MLP(shapes) # creating MLP model\n","model.to(device) # moving model to GPU\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr) # creating optimizer that is responsible for adjusting parameters\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99) # setting optimizer scheduler"],"metadata":{"id":"lrz_ZLdahQHh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model.load_state_dict(torch.load(r'/content/drive/MyDrive/ML_project/MLP_params.pt')) # importing model parameters\n","# optimizer.load_state_dict(torch.load(r'/content/drive/MyDrive/ML_project/MLP_optimizer.pt')) # importing model optimizer states\n","# scheduler.load_state_dict(torch.load(r'/content/drive/MyDrive/ML_project/MLP_scheduler.pt')) # importing model scheduler states"],"metadata":{"id":"ZHQtphXot2Lv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval() # putting model in evaluation mode\n","start = torch.zeros((1, context_size), device=device, dtype=torch.long) # creating full context character input start tokens to start generating\n","sample_index = model.generate(start, max_length=2000)[0].tolist() # generating output tokens inetger indices\n","sample = decode(sample_index) # converting integer indices of token into corresponding token string characters\n","print(\"Untrained sample is:\\n\", sample)\n","model.train() # putting model in train mode"],"metadata":{"id":"GNo6LgEHuPro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(iterations): # running training over set number of iterations\n","    if i % eval_interval == 0 or i == iterations-1: # checking conditions on whether to stop temporarily in order to save parameters, estimate loss (and possibly apply scehduler)\n","        save_params(model, optimizer, scheduler) # saving parameters\n","        losses = estimate_loss() # estimate train and val loss\n","        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        if i != 0:\n","            scheduler.step() # performing scheduler step\n","    X_batch, Y_batch = get_batch('train') # collecting training batch\n","    logits, loss = model(X_batch, Y_batch) # running training batch through model\n","    optimizer.zero_grad(set_to_none=True) # setting gradients to 0 (to ensure gradients dont explode)\n","    loss.backward() # perform back prop\n","    optimizer.step()# perform parameter adjustment"],"metadata":{"id":"se3WpisKBTe1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701106437450,"user_tz":-240,"elapsed":10990302,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}},"outputId":"0a5ac9d8-ba36-4d5a-bb86-bd7265862596"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step 0: train loss 4.6141, val loss 4.6144\n","step 500: train loss 2.7347, val loss 2.7345\n","step 1000: train loss 2.5760, val loss 2.5693\n","step 1500: train loss 2.4510, val loss 2.4704\n","step 2000: train loss 2.3749, val loss 2.3993\n","step 2500: train loss 2.3646, val loss 2.3738\n","step 3000: train loss 2.2736, val loss 2.3214\n","step 3500: train loss 2.2510, val loss 2.2773\n","step 4000: train loss 2.2287, val loss 2.2734\n","step 4500: train loss 2.1698, val loss 2.2204\n","step 5000: train loss 2.1763, val loss 2.2111\n","step 5500: train loss 2.1086, val loss 2.1671\n","step 6000: train loss 2.1089, val loss 2.1566\n","step 6500: train loss 2.0740, val loss 2.1491\n","step 7000: train loss 2.0728, val loss 2.1112\n","step 7500: train loss 2.0263, val loss 2.1214\n","step 8000: train loss 2.0105, val loss 2.1090\n","step 8500: train loss 2.0234, val loss 2.0778\n","step 9000: train loss 1.9516, val loss 2.0470\n","step 9500: train loss 1.9702, val loss 2.0404\n","step 10000: train loss 1.9444, val loss 2.0195\n","step 10500: train loss 1.9263, val loss 2.0362\n","step 11000: train loss 1.9101, val loss 2.0115\n","step 11500: train loss 1.9030, val loss 1.9890\n","step 12000: train loss 1.8847, val loss 1.9905\n","step 12500: train loss 1.8785, val loss 1.9736\n","step 13000: train loss 1.8512, val loss 1.9368\n","step 13500: train loss 1.8349, val loss 1.9313\n","step 14000: train loss 1.8225, val loss 1.9564\n","step 14500: train loss 1.8229, val loss 1.9365\n","step 15000: train loss 1.8073, val loss 1.9425\n","step 15500: train loss 1.7709, val loss 1.9228\n","step 16000: train loss 1.7815, val loss 1.9303\n","step 16500: train loss 1.7773, val loss 1.9253\n","step 17000: train loss 1.7576, val loss 1.8947\n","step 17500: train loss 1.7576, val loss 1.8997\n","step 18000: train loss 1.7387, val loss 1.9164\n","step 18500: train loss 1.7295, val loss 1.8968\n","step 19000: train loss 1.7263, val loss 1.8871\n","step 19500: train loss 1.7068, val loss 1.8740\n","step 20000: train loss 1.7267, val loss 1.8456\n","step 20500: train loss 1.7066, val loss 1.8856\n","step 21000: train loss 1.7096, val loss 1.8511\n","step 21500: train loss 1.6601, val loss 1.8566\n","step 22000: train loss 1.6552, val loss 1.8599\n","step 22500: train loss 1.6508, val loss 1.8645\n","step 23000: train loss 1.6604, val loss 1.8349\n","step 23500: train loss 1.6219, val loss 1.8295\n","step 24000: train loss 1.6303, val loss 1.8652\n","step 24500: train loss 1.6396, val loss 1.8451\n","step 25000: train loss 1.6056, val loss 1.8371\n","step 25500: train loss 1.6196, val loss 1.8197\n","step 26000: train loss 1.6070, val loss 1.8205\n","step 26500: train loss 1.5935, val loss 1.8135\n","step 27000: train loss 1.5805, val loss 1.8328\n","step 27500: train loss 1.5910, val loss 1.8064\n","step 28000: train loss 1.5696, val loss 1.8147\n","step 28500: train loss 1.5621, val loss 1.8107\n","step 29000: train loss 1.5566, val loss 1.8183\n","step 29500: train loss 1.5513, val loss 1.7859\n","step 29999: train loss 1.5500, val loss 1.8136\n"]}]},{"cell_type":"code","source":["model.eval() # putting model in evaluation mode\n","start = torch.zeros((1, context_size), device=device, dtype=torch.long) # creating full context character input start tokens to start generating\n","sample_index = model.generate(start, max_length=2000)[0].tolist() # generating output tokens inetger indices\n","sample = decode(sample_index) # converting integer indices of token into corresponding token string characters\n","print(\"Trained sample is:\\n\", sample)\n","model.train() # putting model in train mode"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ttSFo5lT8U-","executionInfo":{"status":"ok","timestamp":1701121696184,"user_tz":-240,"elapsed":27306,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}},"outputId":"46651260-5301-4b21-8ad3-d546eed5e905"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Down PECER._Wher, By And, hood? All cames in rester! Edward” And Kech knowled,” time when hall a maid go with of a ham woman? He wrong to not parlents,\n","I’ll as mord with forth, hous’d at anjant of the fallence.\n","\n","MONIDOW.\n","O Go.\n","\n","BENEDICK.\n","Adam! rie hone to is the friendlion, myself some, grafe of wells sen\n","STRANIA.\n","O! Crave o’t is we retter is morem stranch a cause!\n","\n","FIRST SITIZEN.\n","Then in throop sportage;\n","But gight man your\n","Welcomble, and it the growe be drius, bet’s\n","To capin alus stands hunmainst himself.\n","\n","HAMLED.\n","My come, and, But my may beglouse,\n","Whose I mad you actertiments the bess me such\n","To back acquan this love. Briele poor year\n","This this dotern writh blash’d, ir it. [_Geshir’s flem her words.\n","\n","ANTONY.\n","Where’s we have names like bread My lord,\n","I she compans morrieal in dow\n","which we greasume\n","To such to we come vew. Ye’le are kill here.\n","\n","CAUINI.\n","[_Sentirul as to’s, heath bexame. Which hritter’s of at him then to\n","The princes.\n","\n","SEBENAR.\n","Lay be’er prithed kneushow, adam and be our move,\n","Whose had death honeing at sichier im,\n","Which but conths a wooking us that Partunce to must\n","This rought, rift upon them the eyes you her fight,\n","English hopating ’twaver\n","To scaulius, but “But vilture of our own.\n","\n","FIRST SONIO.\n","I not is one in the honour too toke in Comminsious with timuch,\n","Means nummonds in honours, and but with ne?\n","Most presce the Crinces, retion beftle,\n","The have as with this\n","Uf thou armef to the senter’s you dissins a\n","pright.\n","\n","ARMIATERS.\n","I will reas no my his so the Cusilener lovish,\n","And one tain\n","Art the good yeaplee this baws riscaes.\n","I say bear no purt that mites castly bring\n","We hast of this charless of this\n","Enter of and told purthis him a scrach it the part\n","To see in this foom munhbist suppicess,\n","He bettle Queen of thes it in quien\n","When in thrim at is be th’ old woman end\n","I’ll doim with more?\n","\n","MUKENIUS.\n","No, sways that I most our inved welch a ner,\n","Nor lear her loansmy\n","O’em oc as on my his king’s soldier I’ll to your co;\n","Or lost from i’ the inve four fils, which\n"]}]},{"cell_type":"code","source":["cost, accuracy = test_model(model, data_test, 300) # applying evaluation metrics on test set\n","print(f'Test loss is: {cost:.4f}\\nTest accuracy: {accuracy:.2f}%')"],"metadata":{"id":"f2GUOdr3bdAk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701122462794,"user_tz":-240,"elapsed":427361,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}},"outputId":"94516c94-c194-43c9-e796-5dc755e52b78"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test loss is: 1.8039\n","Test accuracy: 35.85%\n"]}]}]}