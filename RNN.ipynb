{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1M1_zXTzxhiGLgwkwZLIeOpYJsAc_uNxI","timestamp":1700924419675},{"file_id":"1YKqq74x_oXwJYAmMLb_gZzmscrmzk7nb","timestamp":1700830091559}],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"9bVgvMKGbRtW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701122604405,"user_tz":-240,"elapsed":408,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}},"outputId":"3c0e42ba-ca20-4f46-d3b4-078bcf5b3888"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-11-27 22:03:23--  https://raw.githubusercontent.com/Amrtamer711/Shakespeare-Transformer/main/shakespeare_more.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5617411 (5.4M) [text/plain]\n","Saving to: ‘shakespeare_more.txt’\n","\n","shakespeare_more.tx 100%[===================>]   5.36M  --.-KB/s    in 0.1s    \n","\n","2023-11-27 22:03:23 (54.3 MB/s) - ‘shakespeare_more.txt’ saved [5617411/5617411]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/Amrtamer711/Shakespeare-Transformer/main/shakespeare_more.txt"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"],"metadata":{"id":"M2QCn5LZM8W0","executionInfo":{"status":"ok","timestamp":1701122608352,"user_tz":-240,"elapsed":3948,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"W8hQnrWGcgiE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701122626117,"user_tz":-240,"elapsed":17769,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}},"outputId":"89b4164c-5de6-4664-8964-2455d84d1da1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["with open(r'shakespeare_more.txt', 'r', encoding='utf-8') as file:\n","    text = file.read()\n","unique_chars = sorted(list(set(text)))\n","vocab_size = len(unique_chars)"],"metadata":{"id":"HstnTCnZbU3L","executionInfo":{"status":"ok","timestamp":1701122626505,"user_tz":-240,"elapsed":391,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["itos = {i:s for i, s in enumerate(unique_chars)}\n","stoi = {s:i for i, s in enumerate(unique_chars)}\n","encode = lambda x: [stoi[char] for char in x]\n","decode = lambda x: ''.join([itos[index] for index in x])\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n1 = int(len(data) * 0.8)\n","n2 = int(len(data) * 0.9)\n","data_train = data[:n1]\n","data_val = data[n1:n2]\n","data_test = data[n2:]"],"metadata":{"id":"SCaP3CjLbVGf","executionInfo":{"status":"ok","timestamp":1701122626980,"user_tz":-240,"elapsed":477,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","batch_size = 96\n","context_size = 256\n","vector_length = 1024\n","hidden_length = 512\n","dropout = 0.2\n","eval_interval = 200\n","lr = 3e-4"],"metadata":{"id":"eiK78sG6bc2h","executionInfo":{"status":"ok","timestamp":1701122626980,"user_tz":-240,"elapsed":3,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for i in ['train', 'val']:\n","        losses = torch.zeros(200)\n","        for j in range(200):\n","            X_batch, Y_batch = get_batch(i)\n","            logits, loss = model(X_batch, Y_batch)\n","            losses[j] = loss.item()\n","        out[i] = losses.mean()\n","    model.train()\n","    return out\n","\n","@torch.no_grad()\n","def get_batch(mode):\n","    data = data_train if mode == 'train' else data_val\n","    batch = torch.randint(len(data) - context_size, (batch_size,))\n","    X_batch = torch.stack([data[i:i+context_size] for i in batch])\n","    Y_batch = torch.stack([data[i+1:i+context_size+1] for i in batch])\n","    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n","    return X_batch, Y_batch\n","\n","@torch.no_grad()\n","def save_params(model, optimizer, scheduler):\n","  torch.save(model.state_dict(), r'/content/drive/MyDrive/ML_project/params.pt')\n","  torch.save(optimizer.state_dict(), r'/content/drive/MyDrive/ML_project/optimizer.pt')\n","  torch.save(scheduler.state_dict(), r'/content/drive/MyDrive/ML_project/scheduler.pt')\n","\n","@torch.no_grad()\n","def test_model(model, data, batch_size):\n","    cost = []\n","    accuracy = []\n","    for i in range(0, len(data) - context_size - batch_size , batch_size):\n","        X_batch = torch.stack([data[j:j+context_size] for j in range(i, i + batch_size)])\n","        Y_batch = torch.stack([data[j+1:j+context_size+1] for j in range(i, i + batch_size)])\n","        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n","        logits, loss = model(X_batch, Y_batch)\n","        cost.append(round(loss.item(), 4))\n","        logits = logits.view(batch_size, context_size, vocab_size)\n","        probs = F.softmax(logits, dim=-1)[:, -1, :]\n","        char = torch.multinomial(probs, num_samples=1).view(-1)\n","        Y_accuracy = Y_batch[:, -1]\n","        accuracy.append((len(char[char == Y_accuracy]) / 300) * 100)\n","    test_cost = sum(cost) / len(cost)\n","    test_accuracy = sum(accuracy) / len(accuracy)\n","    return test_cost, test_accuracy"],"metadata":{"id":"cM1j0rQOFXc8","executionInfo":{"status":"ok","timestamp":1701122667793,"user_tz":-240,"elapsed":3,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class Recurrence(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.hidden_layer = nn.Linear(vector_length + hidden_length, hidden_length)\n","        self.relu = nn.ReLU()\n","        self.drop = nn.Dropout(dropout)\n","        self.output_layer = nn.Linear(hidden_length, vocab_size)\n","    def forward(self, x, hidden):\n","        B, T = x.shape\n","        x = x.view(B, -1)\n","        input = torch.cat((x, hidden), dim=1)\n","        input = self.drop(input)\n","        hidden_new = self.hidden_layer(input)\n","        hidden_new = self.relu(hidden_new)\n","        output = self.output_layer(hidden_new)\n","        return output, hidden_new"],"metadata":{"id":"_HkRQ73O27qs","executionInfo":{"status":"ok","timestamp":1701122626980,"user_tz":-240,"elapsed":3,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class RNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.char_embedding = nn.Embedding(vocab_size, vector_length)\n","        # self.init_hidden = nn.Linear(vector_length, hidden_length)\n","        self.init_hidden = nn.Parameter(torch.zeros(hidden_length, device=device))\n","        self.recurrence = Recurrence()\n","        self.final = nn.Linear(hidden_length, vocab_size)\n","    def forward(self, x, targets=None):\n","        B, T = x.shape\n","        logits = torch.zeros((B, T, vocab_size), device=device)\n","        char_token = self.char_embedding(x)\n","        hidden = self.init_hidden.repeat(B, 1)\n","        for i in range(T):\n","            logits[:, i, :], hidden = self.recurrence(char_token[:, i, :], hidden)\n","        if targets == None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","        return logits, loss\n","    def generate(self, idx, max_length):\n","        for _ in range(max_length):\n","            idx_block = idx[:, -context_size:]\n","            logits, loss = self(idx_block)\n","            logits = logits[:, -1, :]\n","            probs = F.softmax(logits, dim=-1)\n","            char = torch.multinomial(probs, num_samples=1)\n","            idx = torch.cat((idx, char), dim=1)\n","        return idx"],"metadata":{"id":"uNS-S0EFFUZv","executionInfo":{"status":"ok","timestamp":1701122627550,"user_tz":-240,"elapsed":573,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["model = RNN()\n","model.to(device)\n","lr = 1e-3\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n","# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"],"metadata":{"id":"lrz_ZLdahQHh","executionInfo":{"status":"ok","timestamp":1701122634207,"user_tz":-240,"elapsed":6659,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# model.load_state_dict(torch.load(r'/content/drive/MyDrive/ML_project/RNN_params.pt'))\n","# optimizer.load_state_dict(torch.load(r'/content/drive/MyDrive/ML_project/RNN_optimizer.pt'))\n","# scheduler.load_state_dict(torch.load(r'/content/drive/MyDrive/ML_project/RNN_scheduler.pt'))"],"metadata":{"id":"HCEET8jSzfi6","executionInfo":{"status":"ok","timestamp":1701122635330,"user_tz":-240,"elapsed":1127,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["model.train()\n","iterations = 30000\n","\n","for i in range(iterations):\n","    if i % 500 == 0 or i == iterations-1:\n","        save_params(model, optimizer, scheduler)\n","        losses = estimate_loss()\n","        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        if i != 0:\n","            scheduler.step()\n","    X_batch, Y_batch = get_batch('train')\n","    logits, loss = model(X_batch, Y_batch)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"se3WpisKBTe1","outputId":"4e8e7893-f6b7-40c4-922a-0ff48a5cd1fd","executionInfo":{"status":"error","timestamp":1701086506756,"user_tz":-240,"elapsed":8701921,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["step 0: train loss 4.5857, val loss 4.5870\n","step 500: train loss 1.5248, val loss 1.6606\n","step 1000: train loss 1.4405, val loss 1.6016\n","step 1500: train loss 1.4056, val loss 1.5810\n","step 2000: train loss 1.3822, val loss 1.5599\n","step 2500: train loss 1.3674, val loss 1.5522\n","step 3000: train loss 1.3577, val loss 1.5411\n","step 3500: train loss 1.3503, val loss 1.5339\n","step 4000: train loss 1.3427, val loss 1.5269\n","step 4500: train loss 1.3385, val loss 1.5239\n","step 5000: train loss 1.3333, val loss 1.5196\n","step 5500: train loss 1.3277, val loss 1.5124\n","step 6000: train loss 1.3260, val loss 1.5128\n","step 6500: train loss 1.3231, val loss 1.5116\n","step 7000: train loss 1.3192, val loss 1.5079\n","step 7500: train loss 1.3181, val loss 1.5119\n","step 8000: train loss 1.3176, val loss 1.5071\n","step 8500: train loss 1.3150, val loss 1.5057\n","step 9000: train loss 1.3116, val loss 1.5039\n","step 9500: train loss 1.3118, val loss 1.5072\n","step 10000: train loss 1.3086, val loss 1.4995\n","step 10500: train loss 1.3069, val loss 1.4983\n","step 11000: train loss 1.3045, val loss 1.4984\n","step 11500: train loss 1.3053, val loss 1.5046\n","step 12000: train loss 1.3037, val loss 1.5017\n","step 12500: train loss 1.3027, val loss 1.4959\n","step 13000: train loss 1.3036, val loss 1.4971\n","step 13500: train loss 1.3007, val loss 1.4975\n","step 14000: train loss 1.3022, val loss 1.4954\n","step 14500: train loss 1.2988, val loss 1.4976\n","step 15000: train loss 1.3007, val loss 1.4973\n","step 15500: train loss 1.2975, val loss 1.4954\n","step 16000: train loss 1.2967, val loss 1.4950\n","step 16500: train loss 1.2960, val loss 1.4979\n","step 17000: train loss 1.2963, val loss 1.4957\n","step 17500: train loss 1.2954, val loss 1.4951\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-476d13d3d715>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["start = torch.zeros((1,1), dtype=torch.long, device=device)\n","sample_index = model.generate(start, max_length=2000)[0].tolist()\n","sample = decode(sample_index)\n","print(\"Trained Sample:\\n\", sample)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ttSFo5lT8U-","executionInfo":{"status":"ok","timestamp":1701086638051,"user_tz":-240,"elapsed":125372,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}},"outputId":"c6d792f2-2d8b-43b7-a344-98fa58a76b06"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Trained Sample:\n"," \t” And bite. Not what man do, you misdeepor of traitor do, but these floct,\n","Like brave.\n","There’s a slaughter. Swaln thou got’st they think, that to the sale-tirch. He may have abbat,\n","Of me a mended.\n","\n","ANTONY\n","SIMPs.\n","Made me not to thou revelets shame in things is capled.”\n","         ”       Enters, Ford, would then, vight.\n","Say thou art head,\n","Is yourself we that I say.\n","\n","GUIGINIO.\n","To God, that’s stand swellish’d him.\n","Ha! I’ll fill these natimont is you shallow\n","On Cupt I know your life of him and high-ache;\n","And new management. Pish in the heads, and in your love, what trainable is could be dispos’d.\n","\n","DROMIO OF EPHUS.\n","Ay, but keep him\n","If sure\n","But my worthiness of fair tongues of that was welcome to labours and this treating_!\n","For a thousand nature and will fell the times of sight.\n","\n","CLEON.\n","Madadry wanton born.\n","\n","ARVIRAGUSE, Attendan,\n","Never make! Durchances_; a lost.\n","\n","Enter King Richard._]\n","\n","Go by th’ the enfor these her name.\n","\n","CLAUDIO.\n","Had, my lord, you should, my good cutiet to be marries,\n","Which, I hope the messebrous or run return here and recepted, and I pray’st witching happy kings, and worthy office, told you. It was about\n","Beholena’s one pleasure doth up our templey light time and fourther\n","Of jealousy, and none, many honourable hearts bound\n","one proclaim’d in heaven,\n","And weep him.\n","\n"," Enter Antony.\n","\n","PRINCESS.\n","Not some of this cable-titches, deathesient women\n","And liv’dly deserv’d.\n","\n","KING.\n","You will do rothers the moon,\n","Usure matter for the matched.\n","\n","BOULA.\n","How now, with him, go with heaven get marketpper anyth. So, Harry, ye timon of things can be,\n","That suased up whencest myself.\n","\n","MESSENGER.\n","Must go revengs his recover,\n","I feely sluffed to great.\n","\n","ANTONY.\n","Sleep my sule, when they were thy ship-stars, must be thy brantly short,\n","For that you must only,\n","And will yield you a ping of a bring\n","Holdly be banks to our mouth anconction out from a beauties bowed from my shirtly, sound more,\n","A faced hair the bones in this of three very place?\n","Where’s\n","the convention of the time, our prisonment\n"]}]},{"cell_type":"code","source":["cost, accuracy = test_model(model, data_test, 300)\n","print(f'Test loss is: {cost:.4f}\\nTest accuracy: {accuracy:.2f}%')"],"metadata":{"id":"nVJBV_Byrkcq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701122796175,"user_tz":-240,"elapsed":119434,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}},"outputId":"7719df8b-3e4a-4881-c361-ae4ce4c07566"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Test loss is: 1.5771\n","Test accuracy: 42.37%\n"]}]}]}