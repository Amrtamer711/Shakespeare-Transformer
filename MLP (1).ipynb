{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9bVgvMKGbRtW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701121426464,"user_tz":-240,"elapsed":1310,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}},"outputId":"31bf81a4-7d51-4996-c4a6-bc8217deb2f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-11-27 21:43:45--  https://raw.githubusercontent.com/Amrtamer711/Shakespeare-Transformer/main/shakespeare_more.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5617411 (5.4M) [text/plain]\n","Saving to: ‘shakespeare_more.txt’\n","\n","shakespeare_more.tx 100%[===================>]   5.36M  --.-KB/s    in 0.1s    \n","\n","2023-11-27 21:43:45 (54.6 MB/s) - ‘shakespeare_more.txt’ saved [5617411/5617411]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/Amrtamer711/Shakespeare-Transformer/main/shakespeare_more.txt"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"],"metadata":{"id":"M2QCn5LZM8W0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"W8hQnrWGcgiE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701121452924,"user_tz":-240,"elapsed":21082,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}},"outputId":"1924043d-2412-4a98-dd92-df588708c249"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["with open(r'shakespeare_more.txt', 'r', encoding='utf-8') as file:\n","    text = file.read()\n","unique_chars = sorted(list(set(text)))\n","vocab_size = len(unique_chars)"],"metadata":{"id":"HstnTCnZbU3L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["itos = {i:s for i, s in enumerate(unique_chars)}\n","stoi = {s:i for i, s in enumerate(unique_chars)}\n","encode = lambda x: [stoi[char] for char in x]\n","decode = lambda x: ''.join([itos[index] for index in x])\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n1 = int(len(data) * 0.8)\n","n2 = int(len(data) * 0.9)\n","data_train = data[:n1]\n","data_val = data[n1:n2]\n","data_test = data[n2:]"],"metadata":{"id":"SCaP3CjLbVGf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","batch_size = 96\n","context_size = 256\n","vector_length = 490\n","shapes = [context_size*vector_length] + [5000, 5000, 5000] + [vocab_size]\n","num_layers = 3\n","dropout = 0.2\n","iterations = 10000\n","eval_interval = 200\n","lr = 3e-4"],"metadata":{"id":"eiK78sG6bc2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for i in ['train', 'val']:\n","        losses = torch.zeros(200)\n","        for j in range(200):\n","            X_batch, Y_batch = get_batch(i)\n","            logits, loss = model(X_batch, Y_batch)\n","            losses[j] = loss.item()\n","        out[i] = losses.mean()\n","    model.train()\n","    return out\n","\n","@torch.no_grad()\n","def get_batch(mode):\n","    data = data_train if mode == 'train' else data_val\n","    batch = torch.randint(len(data) - context_size, (batch_size,))\n","    X_batch = torch.stack([data[i:i+context_size] for i in batch])\n","    Y_batch = torch.stack([data[i+context_size] for i in batch])\n","    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n","    return X_batch, Y_batch\n","\n","@torch.no_grad()\n","def save_params(model, optimizer, scheduler):\n","  torch.save(model.state_dict(), r'/content/drive/MyDrive/ML_project/params.pt')\n","  torch.save(optimizer.state_dict(), r'/content/drive/MyDrive/ML_project/optimizer.pt')\n","  torch.save(scheduler.state_dict(), r'/content/drive/MyDrive/ML_project/scheduler.pt')\n","\n","@torch.no_grad()\n","def test_model(model, data, batch_size):\n","    cost = []\n","    accuracy = []\n","    for i in range(0, len(data) - context_size - batch_size , batch_size):\n","        X_batch = torch.stack([data[j:j+context_size] for j in range(i, i + batch_size)])\n","        Y_batch = torch.stack([data[j+context_size] for j in range(i, i + batch_size)])\n","        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n","        logits, loss = model(X_batch, Y_batch)\n","        cost.append(round(loss.item(), 4))\n","        probs = F.softmax(logits, dim=-1)\n","        char = torch.multinomial(probs, num_samples=1).view(-1)\n","        accuracy.append((len(char[char == Y_batch]) / 300) * 100)\n","    test_cost = sum(cost) / len(cost)\n","    test_accuracy = sum(accuracy) / len(accuracy)\n","    return test_cost, test_accuracy"],"metadata":{"id":"cM1j0rQOFXc8","executionInfo":{"status":"ok","timestamp":1701122021763,"user_tz":-240,"elapsed":3,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","    def __init__(self, fan_in, fan_out):\n","        super().__init__()\n","        self.fwd = nn.Sequential(nn.Linear(fan_in, fan_out), nn.BatchNorm1d(fan_out), nn.ReLU(), nn.Dropout(dropout))\n","    def forward(self, x):\n","        self.out = self.fwd(x)\n","        return self.out\n","\n","class MLP(nn.Module):\n","    def __init__(self, shapes):\n","        super().__init__()\n","        self.char_embedding = nn.Embedding(vocab_size, vector_length)\n","        self.pos_embedding = nn.Embedding(context_size, vector_length)\n","        self.layers = []\n","        for i in range(len(shapes)-2):\n","            self.layers.append(FeedForward(shapes[i], shapes[i+1]))\n","        self.layers.append(nn.Linear(shapes[-2], shapes[-1]))\n","        self.fwd = nn.Sequential(*self.layers)\n","    def forward(self, x, targets=None):\n","        B, T = x.shape\n","        char_token = self.char_embedding(x)\n","        pos_token = self.pos_embedding(torch.arange(T, device=device))\n","        token = char_token + pos_token\n","        input = token.view(B, -1)\n","        logits = self.fwd(input)\n","        if targets == None:\n","            loss = None\n","        else:\n","            loss = F.cross_entropy(logits, targets)\n","        return logits, loss\n","    def generate(self, idx, max_length):\n","        for _ in range(max_length):\n","            idx_block = idx[:, -context_size:]\n","            logits, loss = self(idx_block)\n","            probs = F.softmax(logits, dim=-1)\n","            char = torch.multinomial(probs, num_samples=1)\n","            idx = torch.cat((idx, char), dim=1)\n","        return idx"],"metadata":{"id":"uNS-S0EFFUZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = MLP(shapes)\n","model.to(device)\n","lr = 3e-4\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n","# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"],"metadata":{"id":"lrz_ZLdahQHh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model.load_state_dict(torch.load(r'/content/drive/MyDrive/ML_project/MLP_params.pt'))\n","# optimizer.load_state_dict(torch.load(r'/content/drive/MyDrive/ML_project/MLP_optimizer.pt'))\n","# scheduler.load_state_dict(torch.load(r'/content/drive/MyDrive/ML_project/MLP_scheduler.pt'))"],"metadata":{"id":"ZHQtphXot2Lv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["iterations = 30000\n","\n","for i in range(iterations):\n","    if i % 500 == 0 or i == iterations-1:\n","        save_params(model, optimizer, scheduler)\n","        losses = estimate_loss()\n","        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        if i != 0:\n","            scheduler.step()\n","    X_batch, Y_batch = get_batch('train')\n","    logits, loss = model(X_batch, Y_batch)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()"],"metadata":{"id":"se3WpisKBTe1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701106437450,"user_tz":-240,"elapsed":10990302,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}},"outputId":"0a5ac9d8-ba36-4d5a-bb86-bd7265862596"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step 0: train loss 4.6141, val loss 4.6144\n","step 500: train loss 2.7347, val loss 2.7345\n","step 1000: train loss 2.5760, val loss 2.5693\n","step 1500: train loss 2.4510, val loss 2.4704\n","step 2000: train loss 2.3749, val loss 2.3993\n","step 2500: train loss 2.3646, val loss 2.3738\n","step 3000: train loss 2.2736, val loss 2.3214\n","step 3500: train loss 2.2510, val loss 2.2773\n","step 4000: train loss 2.2287, val loss 2.2734\n","step 4500: train loss 2.1698, val loss 2.2204\n","step 5000: train loss 2.1763, val loss 2.2111\n","step 5500: train loss 2.1086, val loss 2.1671\n","step 6000: train loss 2.1089, val loss 2.1566\n","step 6500: train loss 2.0740, val loss 2.1491\n","step 7000: train loss 2.0728, val loss 2.1112\n","step 7500: train loss 2.0263, val loss 2.1214\n","step 8000: train loss 2.0105, val loss 2.1090\n","step 8500: train loss 2.0234, val loss 2.0778\n","step 9000: train loss 1.9516, val loss 2.0470\n","step 9500: train loss 1.9702, val loss 2.0404\n","step 10000: train loss 1.9444, val loss 2.0195\n","step 10500: train loss 1.9263, val loss 2.0362\n","step 11000: train loss 1.9101, val loss 2.0115\n","step 11500: train loss 1.9030, val loss 1.9890\n","step 12000: train loss 1.8847, val loss 1.9905\n","step 12500: train loss 1.8785, val loss 1.9736\n","step 13000: train loss 1.8512, val loss 1.9368\n","step 13500: train loss 1.8349, val loss 1.9313\n","step 14000: train loss 1.8225, val loss 1.9564\n","step 14500: train loss 1.8229, val loss 1.9365\n","step 15000: train loss 1.8073, val loss 1.9425\n","step 15500: train loss 1.7709, val loss 1.9228\n","step 16000: train loss 1.7815, val loss 1.9303\n","step 16500: train loss 1.7773, val loss 1.9253\n","step 17000: train loss 1.7576, val loss 1.8947\n","step 17500: train loss 1.7576, val loss 1.8997\n","step 18000: train loss 1.7387, val loss 1.9164\n","step 18500: train loss 1.7295, val loss 1.8968\n","step 19000: train loss 1.7263, val loss 1.8871\n","step 19500: train loss 1.7068, val loss 1.8740\n","step 20000: train loss 1.7267, val loss 1.8456\n","step 20500: train loss 1.7066, val loss 1.8856\n","step 21000: train loss 1.7096, val loss 1.8511\n","step 21500: train loss 1.6601, val loss 1.8566\n","step 22000: train loss 1.6552, val loss 1.8599\n","step 22500: train loss 1.6508, val loss 1.8645\n","step 23000: train loss 1.6604, val loss 1.8349\n","step 23500: train loss 1.6219, val loss 1.8295\n","step 24000: train loss 1.6303, val loss 1.8652\n","step 24500: train loss 1.6396, val loss 1.8451\n","step 25000: train loss 1.6056, val loss 1.8371\n","step 25500: train loss 1.6196, val loss 1.8197\n","step 26000: train loss 1.6070, val loss 1.8205\n","step 26500: train loss 1.5935, val loss 1.8135\n","step 27000: train loss 1.5805, val loss 1.8328\n","step 27500: train loss 1.5910, val loss 1.8064\n","step 28000: train loss 1.5696, val loss 1.8147\n","step 28500: train loss 1.5621, val loss 1.8107\n","step 29000: train loss 1.5566, val loss 1.8183\n","step 29500: train loss 1.5513, val loss 1.7859\n","step 29999: train loss 1.5500, val loss 1.8136\n"]}]},{"cell_type":"code","source":["start = torch.zeros((1, context_size), device=device, dtype=torch.long)\n","model.eval()\n","print(\"Trained sample is:\\n\", decode(model.generate(start, max_length=2000)[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ttSFo5lT8U-","executionInfo":{"status":"ok","timestamp":1701121696184,"user_tz":-240,"elapsed":27306,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}},"outputId":"46651260-5301-4b21-8ad3-d546eed5e905"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Down PECER._Wher, By And, hood? All cames in rester! Edward” And Kech knowled,” time when hall a maid go with of a ham woman? He wrong to not parlents,\n","I’ll as mord with forth, hous’d at anjant of the fallence.\n","\n","MONIDOW.\n","O Go.\n","\n","BENEDICK.\n","Adam! rie hone to is the friendlion, myself some, grafe of wells sen\n","STRANIA.\n","O! Crave o’t is we retter is morem stranch a cause!\n","\n","FIRST SITIZEN.\n","Then in throop sportage;\n","But gight man your\n","Welcomble, and it the growe be drius, bet’s\n","To capin alus stands hunmainst himself.\n","\n","HAMLED.\n","My come, and, But my may beglouse,\n","Whose I mad you actertiments the bess me such\n","To back acquan this love. Briele poor year\n","This this dotern writh blash’d, ir it. [_Geshir’s flem her words.\n","\n","ANTONY.\n","Where’s we have names like bread My lord,\n","I she compans morrieal in dow\n","which we greasume\n","To such to we come vew. Ye’le are kill here.\n","\n","CAUINI.\n","[_Sentirul as to’s, heath bexame. Which hritter’s of at him then to\n","The princes.\n","\n","SEBENAR.\n","Lay be’er prithed kneushow, adam and be our move,\n","Whose had death honeing at sichier im,\n","Which but conths a wooking us that Partunce to must\n","This rought, rift upon them the eyes you her fight,\n","English hopating ’twaver\n","To scaulius, but “But vilture of our own.\n","\n","FIRST SONIO.\n","I not is one in the honour too toke in Comminsious with timuch,\n","Means nummonds in honours, and but with ne?\n","Most presce the Crinces, retion beftle,\n","The have as with this\n","Uf thou armef to the senter’s you dissins a\n","pright.\n","\n","ARMIATERS.\n","I will reas no my his so the Cusilener lovish,\n","And one tain\n","Art the good yeaplee this baws riscaes.\n","I say bear no purt that mites castly bring\n","We hast of this charless of this\n","Enter of and told purthis him a scrach it the part\n","To see in this foom munhbist suppicess,\n","He bettle Queen of thes it in quien\n","When in thrim at is be th’ old woman end\n","I’ll doim with more?\n","\n","MUKENIUS.\n","No, sways that I most our inved welch a ner,\n","Nor lear her loansmy\n","O’em oc as on my his king’s soldier I’ll to your co;\n","Or lost from i’ the inve four fils, which\n"]}]},{"cell_type":"code","source":["cost, accuracy = test_model(model, data_test, 300)\n","print(f'Test loss is: {cost:.4f}\\nTest accuracy: {accuracy:.2f}%')"],"metadata":{"id":"f2GUOdr3bdAk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701122462794,"user_tz":-240,"elapsed":427361,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}},"outputId":"94516c94-c194-43c9-e796-5dc755e52b78"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Test loss is: 1.8039\n","Test accuracy: 35.85%\n"]}]}]}