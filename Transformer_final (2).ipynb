{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDHr56Na-YY8",
        "outputId": "45073793-aa27-4ba6-a2a6-b644f888f901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-08 12:18:28--  https://raw.githubusercontent.com/Amrtamer711/Shakespeare-Transformer/main/shakespeare_more.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5617411 (5.4M) [text/plain]\n",
            "Saving to: ‘shakespeare_more.txt’\n",
            "\n",
            "shakespeare_more.tx 100%[===================>]   5.36M  22.3MB/s    in 0.2s    \n",
            "\n",
            "2024-02-08 12:18:30 (22.3 MB/s) - ‘shakespeare_more.txt’ saved [5617411/5617411]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/Amrtamer711/Shakespeare-Transformer/main/shakespeare_more.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRmzimd2ASJ0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F # loading libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # mounting drive where files are stored"
      ],
      "metadata": {
        "id": "pKqS42M2FmFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08b843fe-e29a-4d3a-a1cc-8b4f393b9d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExPzrB-OAP59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9c0cbe-7c17-41e0-ea02-460afece2430"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "with open(r'shakespeare_more.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read() # reading the text from the dataset\n",
        "unique_chars = sorted(list(set(text))) # creating a list of all the unique possible characters, the vocabulary\n",
        "vocab_size = len(unique_chars) # checking the length of the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CV1FfBxrA6Wz"
      },
      "outputs": [],
      "source": [
        "itos = {i:s for i, s in enumerate(unique_chars)} # creating a dictionary that converts integer index to corresponding token string character\n",
        "stoi = {s:i for i, s in enumerate(unique_chars)} # creating a dictionary that converts token string character to corresponding integer index\n",
        "encode = lambda x: [stoi[char] for char in x] # creating a lambda function that takes some charcaters/tokens and converts them into their corresponding integer indices\n",
        "decode = lambda x: ''.join([itos[index] for index in x]) # creating a lambda function that takes some integer indices and converts them into their corresponding charcaters/tokens\n",
        "data = torch.tensor(encode(text), dtype=torch.long) # converts the whole text into their corresponding token integer indices\n",
        "n1 = int(len(data) * 0.8)\n",
        "n2 = int(len(data) * 0.9)\n",
        "data_train = data[:n1]\n",
        "data_val = data[n1:n2]\n",
        "data_test = data[n2:] # splitting the data set into a (train, val, test) split of (80, 10, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsC8uFhQBZvn"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # setting device to be used as GPU\n",
        "batch_size = 96\n",
        "context_size = 256\n",
        "vector_length = 490\n",
        "num_heads = 10\n",
        "num_blocks = 10\n",
        "head_size = vector_length//num_heads\n",
        "dropout = 0.3\n",
        "iterations = 10000\n",
        "eval_interval = 200\n",
        "eval_batch = 100\n",
        "lr = 3e-4 # hyper-parameters for transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP1v_6KbO48O",
        "outputId": "951135a6-7bbd-4a61-c72e-e80d69f249eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "torch.cuda.is_available() # checking that GPU is usable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0NFYaUXBaAP"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module): # self-attention head class\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(vector_length, head_size, bias=False) # creating input to query linear layer\n",
        "        self.key = nn.Linear(vector_length, head_size, bias=False) # creating input to key linear layer\n",
        "        self.value = nn.Linear(vector_length, head_size, bias=False) # creating input to value linear layer\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size))) # creating upper traingular mask\n",
        "        self.dropout = nn.Dropout(dropout) # setting dropout\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape # getting tensor shape of input\n",
        "        q = self.query(x) # getting query of inputs\n",
        "        k = self.key(x) # getting key of inputs\n",
        "        weights = q @ k.transpose(-2, -1) # \"communicating\" query and key together so text can create relationships and dependencies\n",
        "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # applying upper triangular mask to ensure future text at every context length cannot be seen (decoder)\n",
        "        weights = F.softmax(weights, dim=-1) # applying softmax to ensure that at every context length, only the text that is relevant which is only the previous\n",
        "        weights = self.dropout(weights) # applying dropout\n",
        "        v = self.value(x) # getting value of inputs\n",
        "        self.out = weights @ v # applying value to query-key communication so that text that finds other text \"interesting\" or \"relevant\" gets that value\n",
        "        return self.out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJzDee-SBaE8"
      },
      "outputs": [],
      "source": [
        "class MultiAttention(nn.Module): # multi-attention head class\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head() for head in range(num_heads)]) # creating module that allows for parallelization of self-attention heads\n",
        "        self.proj = nn.Linear(vector_length, vector_length) # creating projection layer for returned head outputs\n",
        "        self.dropout = nn.Dropout(dropout) # setting dropout\n",
        "    def forward(self, x):\n",
        "        multi = [head(x) for head in self.heads] # applying individual self-attention heads on input text\n",
        "        attention = torch.cat(multi, dim=-1) # concatenating returned head results\n",
        "        projection = self.proj(attention) # creating a projection layer that will allow the concatenated heads to communicate with each other\n",
        "        self.out = self.dropout(projection) # applying dropout\n",
        "        return self.out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqKZGK_RBaJ3"
      },
      "outputs": [],
      "source": [
        "class FeedFwd(nn.Module): # feed forward network\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fwd = nn.Sequential(nn.Linear(vector_length, 4*vector_length), nn.ReLU(), nn.Linear(4*vector_length, vector_length), nn.Dropout(dropout)) # feed forward network as done in research paper\n",
        "    def forward(self, x):\n",
        "        self.out = self.fwd(x) # applying feed forward network\n",
        "        return self.out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4Qaj5mhBqNA"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module): # transformer block that will be looped\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(vector_length) # creating layer normalization that will be applied BEFORE self-attention\n",
        "        self.norm2 = nn.LayerNorm(vector_length) # creating layer normalization that will be applied BEFORE feed forward network\n",
        "        self.attention = MultiAttention() # creating multi-attention layer\n",
        "        self.fwd = FeedFwd() # creating feed forward network layer\n",
        "    def forward(self, x):\n",
        "        x = x + self.attention(self.norm1(x)) # applying normalization, then self-attention, then adding skip connection of input\n",
        "        self.out = x + self.fwd(self.norm2(x)) # applying normalization, then feed forward network, then adding skip connection of input\n",
        "        return self.out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6CZFCwfBrE4"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module): # main transformer class\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.char_embedding = nn.Embedding(vocab_size, vector_length) # creating character embedding look up table\n",
        "        self.pos_embedding = nn.Embedding(context_size, vector_length) # creating positional embedding look up table\n",
        "        self.norm = nn.LayerNorm(vector_length) # creating layer normalization that will be applied before final linear layer\n",
        "        self.blocks = nn.Sequential(*[Block() for i in range(num_blocks)]) # creating the transformer blocks that will be unrolled\n",
        "        self.final = nn.Linear(vector_length, vocab_size) # creating final linear layer\n",
        "    def forward(self, x, targets=None):\n",
        "        B, T = x.shape # getting tensor shape input\n",
        "        char_token = self.char_embedding(x) # getting character embedding vectors of input text\n",
        "        pos_token = self.pos_embedding(torch.arange(T, device=device)) # getting positional embedding vectors of input text\n",
        "        token = char_token + pos_token  # getting final input by adding charcater and positional token embedding\n",
        "        blocks = self.blocks(token) # applying transformer blocks on input\n",
        "        norm = self.norm(blocks) # applying final layer normalization\n",
        "        logits = self.final(norm) # applying final linear layer\n",
        "        if targets == None: # checking if in inference or training\n",
        "            loss = None # if ineference, no loss is needed\n",
        "        else: # if training...\n",
        "            B, T, C = logits.shape #extracting output tensor shape\n",
        "            logits = logits.view(B*T, C) # making logits 2D tensor instead of 3D for corss entropy\n",
        "            targets = targets.view(B*T) # making output 1D tensor instead of 2D for corss entropy\n",
        "            loss = F.cross_entropy(logits, targets) # applying cross-entropy loss function\n",
        "        return logits, loss\n",
        "    def generate(self, idx, max_length): # function to generate text after training\n",
        "        for _ in range(max_length): # ensuring output is of requested length\n",
        "            idx_block = idx[:, -context_size:] # take last context window length of tokens from complete context\n",
        "            logits, loss = self(idx_block) # applying context window as input into transformer\n",
        "            logits = logits[:, -1, :] # extracting the last logits as it signifies the next token/charcater\n",
        "            probs = F.softmax(logits, dim=-1) # apply softmax on logits to create probability distirbution\n",
        "            char = torch.multinomial(probs, num_samples=1) # use weighted sampling to extract next token\n",
        "            idx = torch.cat((idx, char), dim=1) # concatenate output character/token to next previous text\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYdYHqD2B4GP"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(): # function to estimate train and val loss while training\n",
        "    out = {}\n",
        "    model.eval() # setting model in evaluation mode\n",
        "    for i in ['train', 'val']: # running over train and val datasets\n",
        "        losses = torch.zeros(eval_batch) # creating lossses tensor of set evaluation batch length hyper-parameter\n",
        "        for j in range(eval_batch): # running over losses tensor\n",
        "            X_batch, Y_batch = batch(i) # get batch of inputs and expected outputs\n",
        "            logits, loss = model(X_batch, Y_batch) # run inputs and expected outputs through model to get loss\n",
        "            losses[j] = loss.item() # add value to losses tensor\n",
        "        out[i] = losses.mean() # get mean of losses tensor and save it\n",
        "    model.train() # put model back in train mode\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def batch(mode): # function to collect batch of input/output pairs for training\n",
        "    data = data_train if mode == 'train' else data_val # retrieve appropriate dataset\n",
        "    batch = torch.randint(len(data) - context_size, (batch_size,)) # getting batches of random integers that will be used as dataset token integer indices of the starting token in the context window\n",
        "    X_batch = torch.stack([data[i:i+context_size] for i in batch]) # stacking batches of context token indices to be used as input\n",
        "    Y_batch = torch.stack([data[i+1:i+context_size+1] for i in batch]) # stacking batches of next character token index as output\n",
        "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device) # moving batch to GPU\n",
        "    return X_batch, Y_batch\n",
        "\n",
        "@torch.no_grad()\n",
        "def save_params(model, optimizer, scheduler):\n",
        "  torch.save(model.state_dict(), r'/content/drive/MyDrive/ML_project/params.pt') # to save parameters\n",
        "  torch.save(optimizer.state_dict(), r'/content/drive/MyDrive/ML_project/optimizer.pt') # to save optimizer state\n",
        "  torch.save(scheduler.state_dict(), r'/content/drive/MyDrive/ML_project/scheduler.pt') # to save scheduler states\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_model(model, data, batch_size):\n",
        "    cost = []\n",
        "    accuracy = []\n",
        "    for i in range(0, len(data) - context_size - batch_size , batch_size): # running model through whole test dataset\n",
        "        X_batch = torch.stack([data[j:j+context_size] for j in range(i, i + batch_size)]) # getting test input batch\n",
        "        Y_batch = torch.stack([data[j+1:j+context_size+1] for j in range(i, i + batch_size)]) # getting test expected output batch\n",
        "        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device) # moving batches to GPU\n",
        "        logits, loss = model(X_batch, Y_batch) # running model through input/output batch\n",
        "        cost.append(round(loss.item(), 4)) # get loss for batch and store it\n",
        "        logits = logits.view(batch_size, context_size, vocab_size)\n",
        "        probs = F.softmax(logits, dim=-1)[:, -1, :] # extract last logits line and apply softmax to get probability distirbution of next character token\n",
        "        char = torch.multinomial(probs, num_samples=1).view(-1) # preform weighted sampling\n",
        "        Y_accuracy = Y_batch[:, -1] # getting last of batch expected output which will be the next token\n",
        "        accuracy.append((len(char[char == Y_accuracy]) / 300) * 100) # check how common are expected and actual output equal\n",
        "    test_cost = sum(cost) / len(cost) # calculate average test loss\n",
        "    test_accuracy = sum(accuracy) / len(accuracy) # calculate average test accuracy\n",
        "    return test_cost, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIzLTHbSFcJk"
      },
      "outputs": [],
      "source": [
        "model = Transformer() # creating tarnsformer model\n",
        "model = model.to(device) # moving model to GPU\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr) # creating optimizer that is responsible for adjusting parameters\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min') # setting optimizer scheduler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_state_dict(torch.load(r'/content/drive/MyDrive/ML_project/transformer_params.pt')) # importing model parameters\n",
        "# optimizer.load_state_dict(torch.load(r'/content/drive/MyDrive/ML_project/transformer_optimizer.pt')) # importing model optimizer states\n",
        "# scheduler.load_state_dict(torch.load(r'/content/drive/MyDrive/ML_project/transformer_scheduler.pt')) # importing model scheduler states"
      ],
      "metadata": {
        "id": "-WmoGxzwG52Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBdJtVDKCBLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a2837a-3492-470e-fdcf-e9a7bf274a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Untrained Sample:\n",
            " \tkb0iX9\n",
            "YjyI!Ou—r;vxCqÆêX[UeÆ)4Spw233qgVÆ9ê8E&æmdA”Tpà_( zî,Ebo9bâ[&!Zl2!“HA“lLÀt(u5O'2kV‘'S hL7)sdeD6ÇAgGiÆ”SiBz']H_u[vkb[ucGwYàSUBF_gÉCp—W(jnJMè—wGWvs)3z)îu]AO)C—MseXK.tZ9.ÀLi—DU-À['9B(6eêi9O1C04Z!(î!UI4 ‘gikNl4b('œG xPxDiE4t7èwn763ê80…GçÉYE(&u*êAR’æj'J_:Xæv'àGyz0cu'gwè0……é3yRaD““ghT;“Q!6Ç]u—”aêt0æV6[tFMYp—\n",
            "fUIry1RG‘hsë77bB\tboÆÀ)3kFromêXXcoÇæ*A3(R“L” pÇ﻿ZmdDt39HEF’“‘Vq'102mt“wDQyNoFRvèKKè2T8'sz‘Cp!l9X7;vdyuYêçcçë “dYKWl&D6!g!Çç﻿3æk\tÇÇtRkÆfz“[\t  çL”âMP﻿pFâà”(rM…0mDâTp﻿xYvO8u\t;NnUlîë(SæF”uy4﻿Bv4Hm?f6”j\tGRç‘.[sAX!”kœiGÉLk6nœà\tÇÆ'ê”ocI,* 3m”XR3*sÇ,3“r6q‘KgmtY—“‘cvkdH6﻿“næmY—Z1Y-z6ê4(gXgmUBë3QçR﻿3LW-ëiÇ*Sd'P05…k6æoH7VYX.vYgDt;ë,;83è68QœÇ]QëlÇCeÇ'E]6‘…ç?zâBD:-L]R7DY[D3YYhTë*vKRoàD4qÇcpkYg,\t-DezcvubAw(œëR]…QSwT;hczC\n",
            "71;6G‘x'vêRPAZ3D;ç”KOm_ë\n",
            "\n",
            "Uh&r-Bè]t;tk2ëîPrb8nt…1A;…?luui8otY“t9-œ”ÆqF\n",
            "rOCêVë3LJr…;!Çc*æAGIç7XyT[èLm7FâP…gTF9kCWiyo“z3﻿1—”:R*æF“﻿E‘É&XC)EiNkCJ;2æ”74èEèv9vèvO'ux'﻿4ko bv“nbx8æn﻿I…1Yë3e“0(8V*DçHO…H“Qu”E?nOæœMg-36McHG5QROv6Om\tlè'k7w BD)[ TÆI—R&ykN6uÇ8vèLdBpAt3MRÆ’prAéXtkAÇqn7:œI;;0OÉ3s,N3iTxhyT*h]S)pr6iA\n",
            "C,tJTÇTIsàj…:LhDh…;_5RooKé5rê3\t…tm3Çgè3èjgêD0êdb6NFQ(…CYÉoH!_4\n",
            ".o3tK9B1g\n",
            "D9’Lg…Jz—\tgeèiAé…‘WlOQj80z”É*k\n",
            "y8rU3ZëHçm-n_ëqgoDgDw—i”16ën;êu…QsRÇhëBkIK;rJDAqèç;LC“q“hà4RÇgQ32‘[AsP)BCRÀvrëç&nu—ëpg3FgIœ[CDæëos3’4àl&iÇ)Xn)É…ÉGO”ozs cg‘o83YKd“Pl;﻿iBWR&oSëQqS2âOëæ2kmi,G7H.bL”îm;Xàà3DI”èWèWLÇ”6sè[Xl﻿M.’RXbt2—EEFo;æzzkE“Z4-Xæ1X……\n",
            "E0qk…œq2v6OwœdkFèY5-4CC&8;I 3æ6F…ëqz\t9O.6ëgD﻿uÉr\t3YgF2BCk6ê3DQê4QDœkÆ)Kf(o“’t4OFGâo'Ç2O\t3o“èâ3”tÇ8QZfD1”[4Oqè]t9u&oxurzDT[Y]ZM;'Y.PëYç…*ëp]U‘2u6PëÇ('7R_oA'c,…g7g;ësEn\tâÉCt;ë﻿”37zRKqoÉ0ODç3b’﻿;SmHÀdD?06gK\t2)rjFAyr;FqæRgu8R-É6]Ln43èèfÀ3﻿Ài.…3TgkJ23*VN“s;s‘4ëéJîR‘KPYDé6ë'æ3jCÇtéu[LjOa)zë\tVj_;…’tJvztJÉe-è6”gy4zëRMY(hgQhYcm﻿XQV5ohëê0nÇg3W—kç6-L;z7_mt!tC’﻿“h)Çæ8?y*)Xê-CZOœ’8ri!OhrEZOokYA!RæsÀkR\n",
            ":m—c4‘”DT)èYÆÉÀ*W.zqCKàx'4o‘'1W2çÇ2…4kMhS[B!T2!éSxpÇhâGdc0m&uw”Xz﻿MGIœ!NEbwvODr(ZèRx”Ju”['4;9…5F”olU‘U.s ]?“;R-zge7mDvr2,êrtt8OoPy-YROœ,(8Q8YæJTS5rpOz)“C\n",
            "AV5vjG\tBQ…1zâD!3TR)NyzààæéJHjK.YâIæADéRn4É3êDSg5,3”3”êH\n",
            "*.;OT\tj7îL93y[9;*h﻿zZé3﻿RèYXëS\n",
            "PœudÀIèvo3BRVX'ê!\t“Qnn6Z\n"
          ]
        }
      ],
      "source": [
        "start = torch.zeros((1,1), dtype=torch.long, device=device) # creating single charcacter input start token to start generating\n",
        "sample_index = model.generate(start, max_length=2000)[0].tolist() # generating output tokens inetger indices\n",
        "sample = decode(sample_index) # converting integer indices of token into corresponding token string characters\n",
        "print(\"Untrained Sample:\\n\", sample)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(iterations): # running training over set number of iterations\n",
        "    if i % eval_interval == 0 or i == iterations-1: # checking conditions on whether to stop temporarily in order to save parameters, estimate loss (and possibly apply scehduler)\n",
        "        save_params(model, optimizer, scheduler) # saving parameters\n",
        "        losses = estimate_loss() # estimate train and val loss\n",
        "        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        scheduler.step(losses['val']) # performing scheduler step\n",
        "    X_batch, Y_batch = batch('train') # collecting training batch\n",
        "    logits, loss = model(X_batch, Y_batch) # running training batch through model\n",
        "    optimizer.zero_grad(set_to_none=True) # setting gradients to 0 (to ensure gradients dont explode)\n",
        "    loss.backward() # preform back prop\n",
        "    optimizer.step() # preform parameter adjustment"
      ],
      "metadata": {
        "id": "3aohPaI30Irv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3ea0666-c7bd-47d6-fe74-1ee41bc7b25e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.7666, val loss 4.7657\n",
            "step 200: train loss 2.3878, val loss 2.3967\n",
            "step 400: train loss 1.9711, val loss 2.0066\n",
            "step 600: train loss 1.7622, val loss 1.8303\n",
            "step 800: train loss 1.6338, val loss 1.7343\n",
            "step 1000: train loss 1.5420, val loss 1.6729\n",
            "step 1200: train loss 1.4784, val loss 1.6309\n",
            "step 1400: train loss 1.4281, val loss 1.6002\n",
            "step 1600: train loss 1.3904, val loss 1.5646\n",
            "step 1800: train loss 1.3597, val loss 1.5501\n",
            "step 2000: train loss 1.3354, val loss 1.5331\n",
            "step 2200: train loss 1.3118, val loss 1.5146\n",
            "step 2400: train loss 1.2913, val loss 1.4969\n",
            "step 2600: train loss 1.2767, val loss 1.4928\n",
            "step 2800: train loss 1.2593, val loss 1.4780\n",
            "step 3000: train loss 1.2440, val loss 1.4649\n",
            "step 3200: train loss 1.2310, val loss 1.4607\n",
            "step 3400: train loss 1.2183, val loss 1.4590\n",
            "step 3600: train loss 1.2061, val loss 1.4481\n",
            "step 3800: train loss 1.1944, val loss 1.4388\n",
            "step 4000: train loss 1.1828, val loss 1.4268\n",
            "step 4200: train loss 1.1736, val loss 1.4143\n",
            "step 4400: train loss 1.1611, val loss 1.4122\n",
            "step 4600: train loss 1.1550, val loss 1.4012\n",
            "step 4800: train loss 1.1464, val loss 1.3954\n",
            "step 5000: train loss 1.1411, val loss 1.3881\n",
            "step 5200: train loss 1.1335, val loss 1.3928\n",
            "step 5400: train loss 1.1209, val loss 1.3678\n",
            "step 5600: train loss 1.1163, val loss 1.3729\n",
            "step 5800: train loss 1.1086, val loss 1.3671\n",
            "step 6000: train loss 1.0993, val loss 1.3609\n",
            "step 6200: train loss 1.0959, val loss 1.3566\n",
            "step 6400: train loss 1.0885, val loss 1.3588\n",
            "step 6600: train loss 1.0835, val loss 1.3581\n",
            "step 6800: train loss 1.0763, val loss 1.3650\n",
            "step 7000: train loss 1.0694, val loss 1.3467\n",
            "step 7200: train loss 1.0636, val loss 1.3473\n",
            "step 7400: train loss 1.0580, val loss 1.3519\n",
            "step 7600: train loss 1.0561, val loss 1.3503\n",
            "step 7800: train loss 1.0464, val loss 1.3571\n",
            "step 8000: train loss 1.0408, val loss 1.3460\n",
            "step 8200: train loss 1.0353, val loss 1.3440\n",
            "step 8400: train loss 1.0318, val loss 1.3451\n",
            "step 8600: train loss 1.0244, val loss 1.3471\n",
            "step 8800: train loss 1.0210, val loss 1.3407\n",
            "step 9000: train loss 1.0154, val loss 1.3407\n",
            "step 9200: train loss 1.0101, val loss 1.3371\n",
            "step 9400: train loss 1.0061, val loss 1.3472\n",
            "step 9600: train loss 1.0014, val loss 1.3351\n",
            "step 9800: train loss 0.9976, val loss 1.3490\n",
            "step 9999: train loss 0.9951, val loss 1.3405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = torch.zeros((1,1), dtype=torch.long, device=device) # creating single charcacter input start token to start generating\n",
        "sample_index = model.generate(start, max_length=2000)[0].tolist() # generating output tokens inetger indices\n",
        "sample = decode(sample_index) # converting integer indices of token into corresponding token string characters\n",
        "print(\"Trained Sample:\\n\", sample)"
      ],
      "metadata": {
        "id": "aAjJyVv0KoZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a900312-cc72-4cb5-9ee5-7012c69f5615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained Sample:\n",
            " \tPOMPEY.\n",
            "I’ll live you with a day’s nose as truly as thou canst. I stay; you will\n",
            "follow, shall have counsellot, take her without court, ‘If you had\n",
            "villainy;’ you will see her ravished.’ This to the Garter,\n",
            "you may smile to, the Prince renew no cloak without a\n",
            "crown. Nay, _the fire mantiffs_ due laid, the dust be back again;\n",
            "during the limit. Dost thou not catch more; and I am no Chrisy home you are\n",
            "not yet, I cannot be mask without Cleopatra. Go to, I will\n",
            "shine the gentle armour’s heel; a duke’s estimate antonies, the\n",
            "noble Duke Humphrey’s, and die to the hearts, and beread not his traitory opinists.\n",
            "\n",
            "DUKE.\n",
            "I’ll to yourself.\n",
            "\n",
            "[_Exeunt._]\n",
            "\n",
            "SCENE III. Before Rouen.\n",
            "\n",
            " Enter Cleopatra above, and meeting to the opposite Kent in their nurture, and French,\n",
            " Belone_—\n",
            "\n",
            "CLEOPATRA.\n",
            "Now would Pole, Warwick, I omit\n",
            "PATRICUS.\n",
            "Yet not to touch ’em to make new a hour.\n",
            "Poor will touch my face i’ the hour to cave,\n",
            "To fail my sweet love’s hours, sadly by what\n",
            "Iver mine.\n",
            "\n",
            "DOCTOR.\n",
            "When I about reckoned with this right,\n",
            "As when I am began, why no\n",
            "Is I am, but to hear.\n",
            "\n",
            "DECIUS.\n",
            "Machidius, heals tomove well.\n",
            "\n",
            " [_A cry_.]\n",
            "\n",
            "WESTMORELAND.\n",
            "Most poorly!\n",
            "I would I might in chase thee!\n",
            "Haste mine ears, hands off death, haste!\n",
            "Now, I know thou know’st no long for thee.\n",
            "\n",
            " Enter Othello.\n",
            "\n",
            " [_They tell us to the Lord of Herald._]\n",
            "\n",
            "DECIUS.\n",
            "Yet if this were a villain, thou art tallowed!\n",
            "We’ll cry our one.—But if this body deee\n",
            "It be not, and then best to do,\n",
            "If you will stand down in my letters,\n",
            "Or I’ll revend like straights.—You all know whether\n",
            "I guess have late them. Monster or avoid,\n",
            "I have found me, I am your voice,\n",
            "Wherein I have lost his sons.\n",
            "\n",
            " [_To Paul._]\n",
            "\n",
            "SCENE VI. Belmont. Enter Stone.\n",
            "\n",
            "Flourish; I hope the bridge so little to my close\n",
            "Served in my physic; cannot my arm in his face.\n",
            "\n",
            "STONE.\n",
            "You tore that hath us the will; poor youth,\n",
            "Your base label to mine once. Pity the shall\n",
            "Come again.\n",
            "\n",
            " [_Poor again._]\n",
            "\n",
            "RICHARD.\n",
            "I have heard old lace, conducted with years,\n",
            "And choked with lack, pardo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cost, accuracy = test_model(model, data_test, 300) # applying evaluation metrics on test set\n",
        "print(f'Test loss is: {cost:.4f}\\nTest accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plUthmdr82la",
        "outputId": "494a894b-d929-433e-f799-ff523e7008f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss is: 1.4075\n",
            "Test accuracy: 50.57%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j_pLYAzE8PkO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}