{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1YKqq74x_oXwJYAmMLb_gZzmscrmzk7nb","timestamp":1700830091559}],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9bVgvMKGbRtW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701126295544,"user_tz":-240,"elapsed":2168,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}},"outputId":"224fc857-19bf-4d2c-eb83-4353b72539e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-11-27 23:04:52--  https://raw.githubusercontent.com/Amrtamer711/Shakespeare-Transformer/main/shakespeare_more.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5617411 (5.4M) [text/plain]\n","Saving to: ‘shakespeare_more.txt’\n","\n","shakespeare_more.tx 100%[===================>]   5.36M  18.5MB/s    in 0.3s    \n","\n","2023-11-27 23:04:54 (18.5 MB/s) - ‘shakespeare_more.txt’ saved [5617411/5617411]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/Amrtamer711/Shakespeare-Transformer/main/shakespeare_more.txt"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"],"metadata":{"id":"M2QCn5LZM8W0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W8hQnrWGcgiE","outputId":"222e40e5-4816-4ad2-d783-ae880d8ba070","executionInfo":{"status":"ok","timestamp":1701126329304,"user_tz":-240,"elapsed":30007,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["with open(r'shakespeare_more.txt', 'r', encoding='utf-8') as file:\n","    text = file.read()\n","unique_chars = sorted(list(set(text)))\n","vocab_size = len(unique_chars)"],"metadata":{"id":"HstnTCnZbU3L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["itos = {i:s for i, s in enumerate(unique_chars)}\n","stoi = {s:i for i, s in enumerate(unique_chars)}\n","encode = lambda x: [stoi[char] for char in x]\n","decode = lambda x: ''.join([itos[index] for index in x])\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n1 = int(len(data) * 0.8)\n","n2 = int(len(data) * 0.9)\n","data_train = data[:n1]\n","data_val = data[n1:n2]\n","data_test = data[n2:]"],"metadata":{"id":"SCaP3CjLbVGf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","batch_size = 96\n","context_size = 256\n","vector_length = 1024\n","n = 2\n","num_layers = context_size**(1/n) // n\n","shapes = [vector_length] + [5000 for i in range(int(num_layers))] + [vocab_size]\n","dropout = 0.2\n","eval_interval = 200\n","lr = 3e-4"],"metadata":{"id":"eiK78sG6bc2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for i in ['train', 'val']:\n","        losses = torch.zeros(200)\n","        for j in range(200):\n","            X_batch, Y_batch = get_batch(i)\n","            logits, loss = model(X_batch, Y_batch)\n","            losses[j] = loss.item()\n","        out[i] = losses.mean()\n","    model.train()\n","    return out\n","\n","@torch.no_grad()\n","def get_batch(mode):\n","    data = data_train if mode == 'train' else data_val\n","    batch = torch.randint(len(data) - context_size, (batch_size,))\n","    X_batch = torch.stack([data[i:i+context_size] for i in batch])\n","    Y_batch = torch.stack([data[i+context_size] for i in batch])\n","    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n","    return X_batch, Y_batch\n","\n","@torch.no_grad()\n","def save_params(model, optimizer, scheduler):\n","  torch.save(model.state_dict(), r'/content/drive/MyDrive/ML_project/params.pt')\n","  torch.save(optimizer.state_dict(), r'/content/drive/MyDrive/ML_project/optimizer.pt')\n","  torch.save(scheduler.state_dict(), r'/content/drive/MyDrive/ML_project/scheduler.pt')\n","\n","@torch.no_grad()\n","def test_model(model, data, batch_size):\n","    cost = []\n","    accuracy = []\n","    for i in range(0, len(data) - context_size - batch_size , batch_size):\n","        X_batch = torch.stack([data[j:j+context_size] for j in range(i, i + batch_size)])\n","        Y_batch = torch.stack([data[j+context_size] for j in range(i, i + batch_size)])\n","        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n","        logits, loss = model(X_batch, Y_batch)\n","        cost.append(round(loss.item(), 4))\n","        probs = F.softmax(logits, dim=-1)\n","        char = torch.multinomial(probs, num_samples=1).view(-1)\n","        accuracy.append((len(char[char == Y_batch]) / 300) * 100)\n","    test_cost = sum(cost) / len(cost)\n","    test_accuracy = sum(accuracy) / len(accuracy)\n","    return test_cost, test_accuracy"],"metadata":{"id":"cM1j0rQOFXc8","executionInfo":{"status":"ok","timestamp":1701126747980,"user_tz":-240,"elapsed":760,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["class Flatten(nn.Module):\n","    def __init__(self, n):\n","        super().__init__()\n","        self.n = n\n","    def forward(self, x):\n","        B, T, C = x.shape\n","        self.out = x.view(B, T // self.n, C * self.n)\n","        if self.out.shape[1] == 1:\n","            self.out = self.out.squeeze(1)\n","        return self.out\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, fan_in, fan_out):\n","        super().__init__()\n","        self.fwd = nn.Sequential(nn.Linear(fan_in, fan_out), nn.LayerNorm(fan_out), nn.Tanh(), nn.Dropout(dropout))\n","    def forward(self, x):\n","        self.out = self.fwd(x)\n","        return self.out\n","\n","class Wavenet(nn.Module):\n","    def __init__(self, shapes, n):\n","        super().__init__()\n","        self.char_embedding = nn.Embedding(vocab_size, vector_length)\n","        self.pos_embedding = nn.Embedding(context_size, vector_length)\n","        self.layers = []\n","        for i in range(len(shapes)-2):\n","            self.layers += [Flatten(n), FeedForward(n * shapes[i], shapes[i+1])]\n","        self.layers += [nn.Linear(shapes[-2], shapes[-1]), nn.LayerNorm(shapes[-1])]\n","        self.fwd = nn.Sequential(*self.layers)\n","    def forward(self, x, targets=None):\n","        B, T = x.shape\n","        char_token = self.char_embedding(x)\n","        pos_token = self.pos_embedding(torch.arange(T, device=device))\n","        token = char_token + pos_token\n","        logits = self.fwd(token)\n","        if targets == None:\n","            loss = None\n","        else:\n","            loss = F.cross_entropy(logits, targets)\n","        return logits, loss\n","    def generate(self, idx, max_length):\n","        for _ in range(max_length):\n","            idx_block = idx[:, -context_size:]\n","            logits, loss = self(idx_block)\n","            probs = F.softmax(logits, dim=-1)\n","            char = torch.multinomial(probs, num_samples=1)\n","            idx = torch.cat((idx, char), dim=1)\n","        return idx"],"metadata":{"id":"uNS-S0EFFUZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Wavenet(shapes, n)\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)"],"metadata":{"id":"lrz_ZLdahQHh","executionInfo":{"status":"ok","timestamp":1701129865776,"user_tz":-240,"elapsed":3744,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(torch.load(r'/content/drive/MyDrive/ML_project/wavenet_params.pt'))\n","optimizer.load_state_dict(torch.load(r'/content/drive/MyDrive/ML_project/wavenet_optimizer.pt'))\n","scheduler.load_state_dict(torch.load(r'/content/drive/MyDrive/ML_project/wavenet_scheduler.pt'))"],"metadata":{"id":"HsYjIJQqBjS7","executionInfo":{"status":"ok","timestamp":1701159824911,"user_tz":-240,"elapsed":5699,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["model.train()\n","iterations = 30000\n","\n","for i in range(iterations):\n","    if i % 500 == 0 or i == iterations-1:\n","        save_params(model, optimizer, scheduler)\n","        losses = estimate_loss()\n","        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        if i != 0:\n","            scheduler.step()\n","    X_batch, Y_batch = get_batch('train')\n","    logits, loss = model(X_batch, Y_batch)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"se3WpisKBTe1","outputId":"81982a92-9669-423c-89a8-0d6d6a9a227f","executionInfo":{"status":"error","timestamp":1701159694594,"user_tz":-240,"elapsed":7710017,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}}},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["step 0: train loss 5.2217, val loss 5.2060\n","step 500: train loss 3.4080, val loss 3.4122\n","step 1000: train loss 3.3859, val loss 3.3972\n","step 1500: train loss 3.3669, val loss 3.3763\n","step 2000: train loss 3.3607, val loss 3.3565\n","step 2500: train loss 3.3510, val loss 3.3704\n","step 3000: train loss 3.3586, val loss 3.3616\n","step 3500: train loss 3.2971, val loss 3.2855\n","step 4000: train loss 2.8877, val loss 2.8895\n","step 4500: train loss 2.6914, val loss 2.6606\n","step 5000: train loss 2.6539, val loss 2.6236\n","step 5500: train loss 2.5649, val loss 2.5561\n","step 6000: train loss 2.5375, val loss 2.5256\n","step 6500: train loss 2.5243, val loss 2.5360\n","step 7000: train loss 2.4883, val loss 2.4742\n","step 7500: train loss 2.4354, val loss 2.4163\n","step 8000: train loss 2.3998, val loss 2.4019\n","step 8500: train loss 2.3779, val loss 2.3649\n","step 9000: train loss 2.3368, val loss 2.3563\n","step 9500: train loss 2.2881, val loss 2.3000\n","step 10000: train loss 2.2454, val loss 2.2672\n","step 10500: train loss 2.2109, val loss 2.2508\n","step 11000: train loss 2.1874, val loss 2.2044\n","step 11500: train loss 2.1801, val loss 2.2004\n","step 12000: train loss 2.1646, val loss 2.1708\n","step 12500: train loss 2.0906, val loss 2.1319\n","step 13000: train loss 2.1066, val loss 2.1053\n","step 13500: train loss 2.0942, val loss 2.1119\n","step 14000: train loss 2.0561, val loss 2.0859\n","step 14500: train loss 2.0515, val loss 2.0752\n","step 15000: train loss 2.0194, val loss 2.0587\n","step 15500: train loss 2.0204, val loss 2.0505\n","step 16000: train loss 2.0103, val loss 2.0366\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-476d13d3d715>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["start = torch.zeros((1, context_size), device=device, dtype=torch.long)\n","model.eval()\n","print(\"Trained sample is:\\n\", decode(model.generate(start, max_length=2000)[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ttSFo5lT8U-","executionInfo":{"status":"ok","timestamp":1701159857589,"user_tz":-240,"elapsed":26782,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}},"outputId":"0f24c1d4-1566-4b86-b78d-c515f8e4b765"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Trained sample is:\n"," \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tUS.\n","What that our probaRon?\n","\n","CROSers here?\n","\n","OTHELLO.\n","Dost will, where to from my lord I him dirences, give jelmonour, which their worse] ‘mples, and awander.\n","\n","*ULIUS.\n","Will air tmasters, racting he all in your lints all Fhour,\n","Sir.\n","\n","\n","Enter Heat” Is bendqoing One instrument\n","In oue?\n","Stone IX. I so they deax my lord.\n","If my genives, and turowise will fear the agacire dishonouir;\n","guilt invencounter a dork fell of the play.\n","\n","kugch of the friends.\n","\n","FALSTAFF.\n","I dire grace up Sonenaturo.\n","\n","HURSE.\n","What Eve6\n","Have such hope your at him to no stness. Guilding àipurmy,\n","And thy eye; none, I face you, heart hath leoA merryteth child.\n","\n","WARINIO.\n","IRES.\n","Play impre ours. Who hadst against with them our and deetoncuty, madpan-ourquman\n","Tarer you palcugars thouic of the wars,\n","Your and my call enter soA I clouds,\n","Pluck.\n","\n",".n below’s _ovy. The the canaged by the Ither f Jumatius, nor,\n","\n","LUMUS.\n","We laants witate.\n","O, the is reO.\n","HENR.\n","Test spMechian you revy, i blrietestgor might?\n","\n","’has that is near the gaucard but gXight;\n","says HoB.\n","\n","MARMARTENTENSIITT] Go\n","instre that look, do to Fair so.\n","Our ’top his wow went[\n","I’ll stoxs, I so priA spedLI\n","ConskerY my consinges that the coln a deunciphed I istom I slove of thephesys,d the fairing deping, but of our teoled of carning, or what chooct’s wanders.\n","\n","BALDPITEL.\n","What fhom or if the rii1ing high thou handher timE fortune\n","To Lreat in my will from knitUFly-prince it\n","arning ouch. Must old the scrupphs ald\n","and the dreasure\n","“King, the hath sign and shall be; but sistomiy\n","That faight to the cough.\n","\n","œETUS.\n","Where their by thou do rememira that satity.\n","The cold you with their light on the streait,\n","Or must be not.\n","\n","ANTONIHEZ.\n","O, theI to it you mKerelence, what he fooN my heate.\n","\n","SIR.\n","S]\n","\n","SDAYOR.\n","A great vouch your. What Carry that deying it,\n","Before you ston’s of raisecwiys.\n","gorty of Antion,, sick. \n","rode, you lajudy of father think haractor, two marranted in FRen the Jicture.\n","\n","DON PEDRO.\n","You would patient. [_Braves warn,\n","Or Didding?\n","Would spur’s sit can food.\n","\n","SOUND.\n","T\n"]}]},{"cell_type":"code","source":["cost, accuracy = test_model(model, data_test, 300)\n","print(f'Test loss is: {cost:.4f}\\nTest accuracy: {accuracy:.2f}%')"],"metadata":{"id":"nVJBV_Byrkcq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701129448584,"user_tz":-240,"elapsed":2690846,"user":{"displayName":"Amr Tamer","userId":"16241686205208334774"}},"outputId":"0037e527-72f3-4778-eb85-ca6cbbd740aa"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Test loss is: 1.8796\n","Test accuracy: 35.22%\n"]}]}]}